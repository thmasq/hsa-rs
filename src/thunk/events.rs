#![allow(clippy::cast_possible_truncation)]

use crate::error::{HsaError, HsaResult};
use crate::kfd::device::KfdDevice;
use crate::kfd::ioctl::{
    AMDKFD_IOC_CREATE_EVENT, AMDKFD_IOC_DESTROY_EVENT, AMDKFD_IOC_RESET_EVENT,
    AMDKFD_IOC_SET_EVENT, AMDKFD_IOC_WAIT_EVENTS, CreateEventArgs, DestroyEventArgs,
    EventData as IoctlEventData, KFD_IOC_WAIT_RESULT_TIMEOUT, ResetEventArgs, SetEventArgs,
    WaitEventsArgs,
};
use crate::kfd::sysfs::HsaNodeProperties;
use crate::thunk::memory::{Allocation, MemoryManager};
use std::collections::HashMap;
use std::os::fd::RawFd;
use std::sync::Mutex;
use std::sync::atomic::{AtomicU64, Ordering};
use std::{mem, ptr};

/// The hardware limit for signal events per process.
pub const KFD_SIGNAL_EVENT_LIMIT: usize = 4096;

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
#[repr(u32)]
pub enum HsaEventType {
    Signal = 0,
    NodeChange = 1,
    DeviceStateChange = 2,
    HwException = 3,
    SystemEvent = 4,
    DebugEvent = 5,
    ProfileEvent = 6,
    QueueEvent = 7,
    Memory = 8,
}

impl HsaEventType {
    /// System events are generated by the hardware/driver and cannot be manually signaled by userspace.
    #[must_use]
    pub const fn is_system_event(&self) -> bool {
        !matches!(self, Self::Signal | Self::DebugEvent)
    }
}

/// Represents the user-space sync variable associated with an event.
#[derive(Debug, Clone, Copy)]
#[repr(C)]
pub struct HsaSyncVar {
    pub user_data: *mut std::ffi::c_void,
    pub sync_var_size: u64,
}

unsafe impl Send for HsaSyncVar {}
unsafe impl Sync for HsaSyncVar {}

/// Descriptor used to create a new event.
#[derive(Debug, Clone)]
pub struct HsaEventDescriptor {
    pub event_type: HsaEventType,
    pub node_id: u32,
    pub sync_var: HsaSyncVar,
}

#[derive(Debug, Clone)]
pub enum HsaEventDataPayload {
    /// Standard payload for Signal events (`SyncVar`).
    SyncVar(HsaSyncVar),
    /// Payload for Memory Exception events.
    MemoryAccessFault(HsaMemoryAccessFault),
    /// Payload for Hardware Exception events.
    HwException(HsaHwException),
    /// Placeholder for other events (`NodeChange`, `DeviceStateChange`, etc.).
    None,
}

unsafe impl Send for HsaEventDataPayload {}
unsafe impl Sync for HsaEventDataPayload {}

/// Details of a memory access fault reported by the GPU.
#[derive(Debug, Clone)]
pub struct HsaMemoryAccessFault {
    /// The logical Node ID where the fault occurred.
    pub node_id: u32,
    /// The virtual address that caused the fault.
    pub virtual_address: u64,
    /// Specific failure flags.
    pub failure: HsaAccessAttributeFailure,
    /// Whether this fault is unrecoverable.
    pub is_fatal: bool,
}

#[allow(clippy::struct_excessive_bools)]
#[derive(Debug, Clone, Copy)]
pub struct HsaAccessAttributeFailure {
    pub not_present: bool,
    pub read_only: bool,
    pub no_execute: bool,
    pub ecc_error: bool,
    pub imprecise: bool,
    pub error_type: u32,
}

/// Details of a hardware exception (e.g., reset).
#[derive(Debug, Clone)]
pub struct HsaHwException {
    pub node_id: u32,
    pub reset_type: u32,
    pub memory_lost: bool,
    pub reset_cause: u32,
}

/// The main Event Object structure.
///
/// This struct maintains state (`last_event_age`) required to correctly detecting
/// signal firing, especially for Auto-Reset events.
#[derive(Debug)]
pub struct HsaEvent {
    pub event_id: u32,
    pub event_type: HsaEventType,
    pub payload: Mutex<HsaEventDataPayload>,

    // Internal hardware data exposed for debugging or IPC
    pub hw_data1: u64, // OsEventHandle
    pub hw_data2: u64, // HWAddress (pointer into events page)
    pub hw_data3: u32, // HWData (trigger data)

    // State tracking
    /// The last known "age" (counter) of this event from the kernel.
    /// If the kernel reports an age > this value, the event has fired.
    pub last_event_age: AtomicU64,
    pub device: KfdDevice,
}

unsafe impl Send for HsaEvent {}
unsafe impl Sync for HsaEvent {}

impl Drop for HsaEvent {
    fn drop(&mut self) {
        // When the generic HsaEvent is dropped (e.g., the last Arc is released),
        // we automatically tell the kernel to destroy the event object.
        // We ignore errors here because we can't panic in Drop.
        let _ = self.device.destroy_event(self.event_id);
    }
}

impl HsaEvent {
    /// Signals the event (sets it to signaled state).
    pub fn set(&self) -> HsaResult<()> {
        if self.event_type.is_system_event() {
            return Err(HsaError::General(
                "Cannot manually set system event.".into(),
            ));
        }
        self.device.set_event(self.event_id).map_err(HsaError::from)
    }

    /// Resets the event (sets it to non-signaled state).
    pub fn reset(&self) -> HsaResult<()> {
        if self.event_type.is_system_event() {
            return Err(HsaError::General(
                "Cannot manually reset system event.".into(),
            ));
        }
        self.device
            .reset_event(self.event_id)
            .map_err(HsaError::from)
    }
}

/// Manages the global context for events, specifically the Events Page.
pub struct EventManager {
    /// The shared memory page used by the GPU to write signal events.
    /// This is allocated once per process/device context.
    events_page: Option<Allocation>,

    /// Mapping from Kernel GPU IDs to Logical Node IDs.
    /// Required to translate `gpu_id` in exception reports back to the user-facing `node_id`.
    gpu_to_node_map: HashMap<u32, u32>,
}

impl EventManager {
    /// Initialize the `EventManager`.
    ///
    /// This builds the `gpu_to_node_map` immediately to ensure O(1) lookups during event waits.
    #[must_use]
    pub fn new(nodes: &[HsaNodeProperties]) -> Self {
        let mut gpu_to_node_map = HashMap::new();
        for (idx, node) in nodes.iter().enumerate() {
            // Map the Kernel GPU ID to the index in the node list (Logical Node ID).
            if node.kfd_gpu_id != 0 {
                gpu_to_node_map.insert(node.kfd_gpu_id, idx as u32);
            }
        }

        Self {
            events_page: None,
            gpu_to_node_map,
        }
    }

    /// Creates a new HSA Event.
    ///
    /// If this is the first signal event created, it automatically allocates the
    /// hardware Events Page on the GPU associated with the event's `node_id`.
    pub fn create_event(
        &mut self,
        device: &KfdDevice,
        memory_manager: &mut MemoryManager,
        drm_fd: RawFd,
        desc: &HsaEventDescriptor,
        manual_reset: bool,
        is_signaled: bool,
    ) -> HsaResult<HsaEvent> {
        // Changed return type
        let mut args = CreateEventArgs {
            event_type: desc.event_type as u32,
            node_id: desc.node_id,
            auto_reset: u32::from(!manual_reset),
            ..Default::default()
        };

        if self.events_page.is_none() {
            let alloc_size = KFD_SIGNAL_EVENT_LIMIT * 8;

            let events_alloc = memory_manager
                .allocate_gtt(device, alloc_size, desc.node_id, drm_fd)
                .map_err(|e| -> HsaError {
                    eprintln!("Failed to allocate events page: {e:?}");
                    e
                })?;

            if !events_alloc.ptr.is_null() {
                unsafe { ptr::write_bytes(events_alloc.ptr, 0, alloc_size) };
            }

            self.events_page = Some(events_alloc);
        }

        if let Some(alloc) = &self.events_page {
            args.event_page_offset = alloc.handle;
        }

        if let Err(e) = unsafe { device.ioctl(AMDKFD_IOC_CREATE_EVENT, &mut args) } {
            eprintln!("AMDKFD_IOC_CREATE_EVENT failed: {e:?}");
            return Err(HsaError::from(e));
        }

        let hw_data2 = if let Some(alloc) = &self.events_page
            && args.event_slot_index < KFD_SIGNAL_EVENT_LIMIT as u32
        {
            let base = alloc.ptr as u64;
            base + (u64::from(args.event_slot_index) * 8)
        } else {
            0
        };

        let event = HsaEvent {
            event_id: args.event_id,
            event_type: desc.event_type,
            payload: std::sync::Mutex::new(HsaEventDataPayload::SyncVar(desc.sync_var)),
            hw_data1: u64::from(args.event_id),
            hw_data2,
            hw_data3: args.event_trigger_data,
            last_event_age: std::sync::atomic::AtomicU64::new(0),
            device: device.clone(),
        };

        if is_signaled && !desc.event_type.is_system_event() {
            let mut set_args = SetEventArgs {
                event_id: args.event_id,
                pad: 0,
            };
            if let Err(e) = unsafe { device.ioctl(AMDKFD_IOC_SET_EVENT, &mut set_args) } {
                self.destroy_event(device, &event).ok();
                return Err(HsaError::from(e));
            }
        }

        Ok(event)
    }

    pub fn destroy_event(&self, device: &KfdDevice, event: &HsaEvent) -> HsaResult<()> {
        let mut args = DestroyEventArgs {
            event_id: event.event_id,
            pad: 0,
        };

        if let Err(e) = unsafe { device.ioctl(AMDKFD_IOC_DESTROY_EVENT, &mut args) } {
            eprintln!("AMDKFD_IOC_DESTROY_EVENT failed: {e:?}");
            return Err(HsaError::from(e));
        }
        Ok(())
    }

    pub fn set_event(&self, device: &KfdDevice, event: &HsaEvent) -> HsaResult<()> {
        if event.event_type.is_system_event() {
            return Err(HsaError::General(
                "Cannot manually set system event.".into(),
            ));
        }

        let mut args = SetEventArgs {
            event_id: event.event_id,
            pad: 0,
        };

        unsafe { device.ioctl(AMDKFD_IOC_SET_EVENT, &mut args)? };
        Ok(())
    }

    pub fn reset_event(&self, device: &KfdDevice, event: &HsaEvent) -> HsaResult<()> {
        if event.event_type.is_system_event() {
            return Err(HsaError::General(
                "Cannot manually reset system event.".into(),
            ));
        }

        let mut args = ResetEventArgs {
            event_id: event.event_id,
            pad: 0,
        };

        unsafe { device.ioctl(AMDKFD_IOC_RESET_EVENT, &mut args)? };
        Ok(())
    }

    /// Waits on one or more events.
    ///
    /// # Arguments
    /// * `events` - A mutable slice of `HsaEvent` references. Mutable because internal state
    ///   (`last_event_age` and exception payloads) is updated.
    /// * `wait_all` - If true, blocks until *all* events are signaled.
    /// * `timeout_ms` - Timeout in milliseconds.
    ///
    /// # Returns
    /// * `Ok(Vec<usize>)`: A list of indices into the `events` slice corresponding to the events
    ///   that were detected as signaled.
    ///   - This handles **Auto-Reset** events correctly by comparing event age.
    ///   - If `wait_all` is true, this will contain all valid indices.
    /// * `Err(HsaError::WaitTimeout)`: If the timeout expires.
    /// * `Err(HsaError::Io)`: If an underlying IOCTL fails.
    pub fn wait_on_multiple_events(
        &self,
        device: &KfdDevice,
        events: &[&HsaEvent],
        wait_all: bool,
        timeout_ms: u32,
    ) -> HsaResult<Vec<usize>> {
        if events.is_empty() {
            return Err(HsaError::General("No events to wait on.".into()));
        }

        let mut ioctl_events: Vec<IoctlEventData> = events
            .iter()
            .map(|e| {
                let mut data: IoctlEventData = unsafe { mem::zeroed() };
                data.event_id = e.event_id;
                data.kfd_event_data_ext = 0;

                if e.event_type == HsaEventType::Signal {
                    data.payload.signal_event_data.last_event_age =
                        e.last_event_age.load(Ordering::Relaxed);
                }
                data
            })
            .collect();

        let mut args = WaitEventsArgs {
            events_ptr: ioctl_events.as_mut_ptr() as u64,
            num_events: ioctl_events.len() as u32,
            wait_for_all: u32::from(wait_all),
            timeout: timeout_ms,
            wait_result: 0,
        };

        unsafe { device.ioctl(AMDKFD_IOC_WAIT_EVENTS, &mut args)? };

        if args.wait_result == KFD_IOC_WAIT_RESULT_TIMEOUT {
            return Err(HsaError::WaitTimeout);
        }

        let mut signaled_indices = Vec::new();

        for (i, ioctl_evt) in ioctl_events.iter().enumerate() {
            let event = events[i];

            match event.event_type {
                HsaEventType::Signal => unsafe {
                    let new_age = ioctl_evt.payload.signal_event_data.last_event_age;
                    let old_age = event.last_event_age.load(Ordering::Relaxed);

                    if new_age > old_age {
                        event.last_event_age.store(new_age, Ordering::Relaxed);
                        signaled_indices.push(i);
                    }
                },

                HsaEventType::Memory => unsafe {
                    if ioctl_evt.payload.memory_exception_data.gpu_id != 0 {
                        let data = &ioctl_evt.payload.memory_exception_data;

                        let node_id = *self.gpu_to_node_map.get(&data.gpu_id).unwrap_or(&0);
                        {
                            let mut payload_guard = event.payload.lock().unwrap();
                            *payload_guard =
                                HsaEventDataPayload::MemoryAccessFault(HsaMemoryAccessFault {
                                    node_id,
                                    virtual_address: data.va,
                                    failure: HsaAccessAttributeFailure {
                                        not_present: data.failure.not_present != 0,
                                        read_only: data.failure.read_only != 0,
                                        no_execute: data.failure.no_execute != 0,
                                        imprecise: data.failure.imprecise != 0,
                                        ecc_error: data.error_type == 1 || data.error_type == 2,
                                        error_type: data.error_type,
                                    },
                                    is_fatal: true,
                                });
                        }
                        signaled_indices.push(i);
                    }
                },
                HsaEventType::HwException => unsafe {
                    if ioctl_evt.payload.hw_exception_data.gpu_id != 0 {
                        let data = &ioctl_evt.payload.hw_exception_data;

                        let node_id = *self.gpu_to_node_map.get(&data.gpu_id).unwrap_or(&0);
                        {
                            let mut payload_guard = event.payload.lock().unwrap();
                            *payload_guard = HsaEventDataPayload::HwException(HsaHwException {
                                node_id,
                                reset_type: data.reset_type,
                                memory_lost: data.memory_lost != 0,
                                reset_cause: data.reset_cause,
                            });
                        }
                        signaled_indices.push(i);
                    }
                },
                _ => {
                    signaled_indices.push(i);
                }
            }
        }

        if signaled_indices.is_empty() && args.wait_result == 0 {
            for (i, event) in events.iter().enumerate() {
                if event.event_type == HsaEventType::Signal {
                    signaled_indices.push(i);
                }
            }
        }

        Ok(signaled_indices)
    }
}
